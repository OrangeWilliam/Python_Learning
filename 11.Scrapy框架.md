[TOC]

# 路径管理与常见报错

* 包，模块等...

* 路径列表：
  * 查看当前路径列表：import sys-->sys.path
  * 路径搜索顺序
    * 当前路径
    * PythonPath路径
    * 虚拟环境路径
    * site-packages第三方库

* Pycharm自动添加路径，设置--Project--Project Structure--Content Root

* 添加路径：sys.path.append("路径")

## 注意事项

1. 确定入口程序
2. 将项目根目录添加到入口程序中：sys.path.append(os.getcwd())
3. 进入到项目根目录下执行命令



# Scrapy框架

## 文档

[链接](https://www.w3cschool.cn/scrapy2_3/)

## 什么是Srcapy

Scrapy是一个应用程序框架，用于对网站进行爬行和提取结构化数据，这些结构化数据可用于各种有用的应用程序，如数据挖掘、信息处理或历史存档。基于`twisted`的异步爬虫框架。

尽管Scrapy最初是为 [web scraping](https://en.wikipedia.org/wiki/Web_scraping) 它还可以用于使用API提取数据（例如 [Amazon Associates Web Services](https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html) ）或者作为一个通用的网络爬虫。

* Scrapy官网：https://scrapy.org/
* 在线阅读：https://www.w3cschool.cn/scrapy2_3/

### Scrapy提供的主要功能

* 具有优先级功能的调度器
* 去重功能
* 并发限制
* ip使用次数限制
* ....

### Scrapy的使用场景

* 不适合的场景：业务非常简单，性能要求一般的；业务非常复杂，有顺序和时间的限制
* 适合的场景：请求数据量大，对性能有一定要求，优先级功能的调度器和去重功能

### Scrapy组件

 <img src='https://img-blog.csdnimg.cn/20190420193224499.png'>

* 引擎(Scrapy Engine)
* Item 管道
* 调度器(Scheduler)
* 下载器(Downloader)
* 爬虫(Spiders)
* 项目管道(Pipeline)
* 下载器中间件(Downloader Middlewares)
* 爬虫中间件(Spider Middlewares)
* 调度中间件(Scheduler Middewares)

#### 数据流的三个路径

路径1：

1. Engine从 Spider处获得爬取请求( Request)；

2. Engine将爬取请求转发给 Scheduler，包装成任务(3)， 用于调度；

路径2：

3. Engine从 Scheduler处获得下一个要爬取的请求；

4. Engine将爬取请求通过中间件发送给 Downloader；

5. 爬取网页后, Downloader形成响应( Response)，通过中间件发给 Engine；

6. Engine将收到的响应通过中间件发送给 Spider处理；

路径3：

7. Spider处理响应后产生爬取项(scraped Item)和新的爬取请求( Requests)给Engine；

8. Engine将爬取项发送给Item Pipeline(框架出口)；

9. Engine将爬取请求发送给Scheduler 。

#### 数据流的出入口

1. Engine控制各模块数据流，不间断从 Scheduler处获得爬取请求，直至请求为空。
2. 框架入口: Spider的初始爬取请求
3. 框架出口: Item Pipeline
4. ENGINE、SCHEDULER、DOWNLOAD模块功能已有实现，SPIDERS、ITEM PIPELINES模块有用户编写（配置）。

## Scrapy使用

### 1.安装

```shell
pip install scrapy
```

### 2.创建

```shell
scrapy startproject [项目名]
```

#### 2.1项目目录结构

* peoject
  * peoject/             # project's Python module, you'll import your code from here        
    * \_\_init\_\_.py         
    * items.py          # 解析后的结构化结果      
    * middlewares.py    # 下载器中间件和爬虫中间件
    * pipelines.py      #  处理items的组件，一般都用于进行数据库操作
    * settings.py       # 配置文件 
    * spiders/          #存放爬虫实例，一个文件夹下可以有多个爬虫实例
      * \_\_init\_\_.py
      * film.py
      * story.py
      * ...
* scrapy.cfg            # 项目配置文件

### 3.启动和Debug

* 命令行

  ```shell
  scrapy crawl [spider_name]
  ```

* 启动脚本

  ```Python
  #run.py
  from scrapy import cmdline
  
  command="scrapy crawl douban".split()
  cmdline.execute(command)
  ```

### 4.设置

* Scrapy中间件
  * 请求头中间件

