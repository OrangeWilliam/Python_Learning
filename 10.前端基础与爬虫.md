[TOC]

# 请求的组成

Python中请求

```python
def req_jd(keyword):
    url="https://search.jd.com/Search"
    params={
        "keyword":keyword
    }
    headers={
        "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
    }
    resp=requests.request("GET",url,params=params,headers=headers)
    print(resp.text) #str类型的响应内容
    resp.content #bytes的响应内容
    resp.json() #获取json格式数据
    
```

* 请求行（request line）

  * 请求方法（method）
  * URL

* 请求头（headers）

  * user-agent：用来指示当前系统信息
  * cookie：用来指示当前用户信息和行为信息
  * 

* 请求体（body）

  * params（严格来说不属于请求体）：实际请求的时候是url的一部分，post请求也可以用params

    * urlencode和urldecode：（请求头中指定的编码格式只对请求体有效，不对params有效）

      ```python
      from urllib.parse import quote,unquote
      
      print(quote("鼠标"))
      print(unquote("xxxxxxxx"))
      
      ```

      

  * data：携带额外的请求信息

# BeautifulSoup

## 安装

```shell
pip install bs4
pip install lxml
```

## 使用

```Python
from bs4 import BeautifulSoup
from urllib.parse import quote,unquote
from reqDemo import search_Douban

def jd_search_parse(html):
    soup=BeautifulSoup(html,'lxml')
    item=soup.select("a[href^='/search']")
    for i in item:
        print(i.text)
    #print(unquote("%E4%BD%A0%E5%A5%BD%E6%9D%8E%E7%84%95%E8%8B%B1"))


if __name__=="__main__":
    html=search_Douban("你好李焕英").text
    jd_search_parse(html)
```

* 直接定位元素：遵循jquery CSS选择器规则
  * `BeautifulSoup.select()`
* 获取兄弟节点(去除空白字符)
  * 上一个节点：`item.pervious_sibling`
  * 下一个节点：`item.next_sibling`
* 获取父子节点(去除空白字符)
  * 父节点：`item.parent`
  * 子节点：`item.children`
* 获取文本
  * `item.text.strip()`
* 获取属性值
  * `item.arrts["xxxx"]`

# 一个又小又全的爬虫项目（详见代码）

* 任务生产者：生成爬虫任务的组件，作用是建立生产者-消费者模型，将生产者和消费者剥离，可以达到程序暂停、重启的功能

* 配置文件：当前爬虫项目的基础配置信息，避免重复修改
* 主函数/调度器：以逻辑控制流协同各个组件，完成爬取工作，具有一定调度功能
* 下载器：用来和目标服务器进行交互，获取数据的组件
* 解析器：用于解析非结构化的页面内容，获取想要的数据
* 存储器：用于持久化解析的文件
  * 数据库
  * 文件，推荐json，也可以使用csv









